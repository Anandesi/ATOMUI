Will  this predict function works on embeddings derived from all these libraries 


import torch
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from torch.utils.data import DataLoader, TensorDataset
from sentence_transformers import SentenceTransformer

class SequenceClassifierWrapper:
    def __init__(self, model_name, batch_size=16, use_sentence_transformer=False, sentence_transformer_model=None):
        """
        Args:
            model_name (str): Hugging Face model name for sequence classification.
            batch_size (int): Batch size for inference.
            use_sentence_transformer (bool): Whether to use Sentence Transformer for embeddings.
            sentence_transformer_model (str): Model name for Sentence Transformer (if applicable).
        """
        self.use_sentence_transformer = use_sentence_transformer
        self.batch_size = batch_size

        if use_sentence_transformer:
            self.embedding_model = SentenceTransformer(sentence_transformer_model or model_name)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.embedding_model = None
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)

def _get_embeddings(self, texts):
    """
    Generate embeddings using Sentence Transformer, last layer, or other methods.

    Args:
        texts (list[str]): Input texts for generating embeddings.

    Returns:
        np.ndarray: Generated embeddings (shape: [num_samples, embedding_dim]).
    """
    if self.use_sentence_transformer:
        return np.array(self.embedding_model.encode(texts, batch_size=self.batch_size))
    else:
        dataloader = self._prepare_data(texts)
        all_embeddings = []
        self.model.eval()
        with torch.no_grad():
            for batch in dataloader:
                input_ids, attention_mask = batch
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
                embeddings = outputs.hidden_states[-1][:, 0, :].cpu().numpy()  # CLS token
                all_embeddings.append(embeddings)
        if all_embeddings:
            return np.concatenate(all_embeddings, axis=0)
        return np.array([]).reshape(0, self.model.config.hidden_size)



def _prepare_data(self, embeddings):
    """
    Prepare a DataLoader using precomputed embeddings.

    Args:
        embeddings (np.ndarray): Precomputed embeddings (shape: [num_samples, embedding_dim]).

    Returns:
        DataLoader: DataLoader wrapping the embeddings.
    """
    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)
    dataset = TensorDataset(embeddings_tensor)
    return DataLoader(dataset, batch_size=self.batch_size)




def predict(self, texts, embeddings=None):
    """
    Predict class labels. Supports custom embeddings, Sentence Transformer embeddings, or last-layer embeddings.

    Args:
        texts (list[str]): Input texts for classification.
        embeddings (np.ndarray or None): Optional precomputed embeddings (shape: [num_samples, embedding_dim]).

    Returns:
        np.ndarray: Predicted class labels.
    """
    # If embeddings are not provided, generate them
    if embeddings is None:
        embeddings = self._get_embeddings(texts)

    # Prepare DataLoader using the embeddings
    dataloader = self._prepare_data(embeddings)
    all_predictions = []

    self.model.eval()
    with torch.no_grad():
        for batch in dataloader:
            embeddings_tensor = batch[0]  # Extract embeddings from the DataLoader
            outputs = self.model(inputs_embeds=embeddings_tensor)
            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()
            all_predictions.append(predictions)

    # Concatenate predictions and return
    if all_predictions:
        return np.concatenate(all_predictions, axis=0)
    return np.array([])  # Return empty array if no data
